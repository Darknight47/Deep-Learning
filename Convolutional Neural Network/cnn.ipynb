{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Task\n",
    "> Creating a CNN model for determining if an image shows a dog OR a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# We’ll use Keras’ ImageDataGenerator to preprocess our images. It allows us to augment our images on-the-fly while our model is still learning. \n",
    "# This can help our model generalize better to new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training set**  \n",
    "Data augmentation is a strategy that enables us to significantly increase the diversity of data available for training models, without actually collecting new data.  \n",
    "Data augmentation techniques such as cropping, padding, and horizontal flipping are used to train a model with slightly modified versions of the original images.  \n",
    "This helps to make the model more robust to variations in the input data.\n",
    "\n",
    "In our case, train_datagen uses data augmentation techniques including:\n",
    "\n",
    "shear_range: This is for randomly applying shearing transformations. A shearing transformation slants the shape of the image.\n",
    "zoom_range: This is for randomly zooming inside pictures.\n",
    "horizontal_flip: This is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal asymmetry (e.g. real-world pictures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('../dataset/training_set',\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test set**  \n",
    "test_datagen doesn’t need these augmentations because it’s only used to preprocess the test set images, not to train the model.  \n",
    "The test data should represent real-world data as closely as possible, and should not be augmented in the same way as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('../dataset/test_set',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Farhad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "\n",
    "# Add the first convolutional layer\n",
    "cnn_model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(64, 64, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add the second convolutional layer\n",
    "cnn_model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add the third convolutional layer\n",
    "cnn_model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the tensor output from the convolutional layers\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# Add a fully connected (dense) layer\n",
    "cnn_model.add(Dense(units=128, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "cnn_model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Layers (Conv2D):** Convolutional layers are the major building blocks used in convolutional neural networks. A convolution is a mathematical operation that merges two sets of information. In the context of a CNN, the convolution is performed on the input data with the use of a filter or kernel (these terms are used interchangeably) to then produce a feature map. We use three convolutional layers to progressively extract higher-level features from the input image. The parameters inside Conv2D are:\n",
    "\n",
    "1. The first parameter (32, 64, 128) is the number of filters that the convolutional layer will learn. Layers early in the network architecture (closer to the actual input image) learn fewer convolutional filters while layers deeper in the network (closer to the output predictions) will learn more filters. This allows the network to learn more complex representations.\n",
    "\n",
    "2. (3, 3) is the size of the filters. Each filter will be a 3x3 matrix which is convolved with the image.\n",
    "\n",
    "3. input_shape=(64, 64, 3) is only needed for the first layer. It specifies the shape of the input image. 64, 64 is the dimension of the image and 3 stands for the three color channels (RGB).\n",
    "\n",
    "4. activation='relu' is the activation function to use. ReLU (Rectified Linear Unit) is a common activation function that outputs the input directly if it is positive, otherwise, it outputs zero.\n",
    "\n",
    "**MaxPooling Layers (MaxPooling2D):** After each convolutional layer, a pooling layer is often added for downsampling, which reduces the spatial size of the representation, reducing the amount of parameters and computation in the network, and hence also helps to control overfitting. MaxPooling takes the maximum value of the area it is applied to.\n",
    "\n",
    "**Flatten Layer (Flatten):** This layer is used to convert the final feature maps into a one-dimensional single vector. This flattening step is needed so that you can make use of fully connected layers (Dense layers) after some convolutional/maxpool layers. It combines all the found local features of the previous convolutional layers.\n",
    "\n",
    "**Dense Layer (Dense):** After the flatten layer, you use a dense layer (also called fully connected layer), which performs classification on the features extracted by the convolutional layers and downsampled by the max-pooling layers. In this dense layer, every node in the layer is connected to every node in the preceding layer.\n",
    "\n",
    "**Activation Functions:** Activation functions are used to introduce non-linearity to the model. Without activation functions, no matter how many layers your neural network has, it would still behave just like a single-layer perceptron because summing these layers would give you just another linear function. relu is used in the hidden layers and is one of the most commonly used activation functions for hidden layers. It helps the model learn complex patterns and does not activate all the neurons at the same time, thus helping in reducing overfitting.\n",
    "\n",
    "**Output Layer:** The last layer is the output layer. It uses the sigmoid activation function, which squashes the output between 0 and 1. This is useful for binary classification as it can be interpreted as the probability of the input image being a dog (or whatever class you define as 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model  \n",
    "Before we can start training, we need to configure the learning process. We do this with the compile method. Here, we need to specify the optimizer, the loss function, and the metrics we want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# In this case, we’re using the Adam optimization algorithm, binary cross-entropy as our loss function (since this is a binary classification problem), and accuracy as our performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Farhad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 168ms/step - accuracy: 0.5155 - loss: 0.6934 - val_accuracy: 0.5835 - val_loss: 0.6589\n",
      "Epoch 2/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6212 - loss: 0.6429 - val_accuracy: 0.6850 - val_loss: 0.6017\n",
      "Epoch 3/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6893 - loss: 0.5821 - val_accuracy: 0.7255 - val_loss: 0.5496\n",
      "Epoch 4/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7104 - loss: 0.5519 - val_accuracy: 0.7620 - val_loss: 0.4932\n",
      "Epoch 5/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7545 - loss: 0.5100 - val_accuracy: 0.7775 - val_loss: 0.4545\n",
      "Epoch 6/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7758 - loss: 0.4721 - val_accuracy: 0.7880 - val_loss: 0.4541\n",
      "Epoch 7/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7775 - loss: 0.4631 - val_accuracy: 0.7660 - val_loss: 0.4671\n",
      "Epoch 8/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7984 - loss: 0.4313 - val_accuracy: 0.8170 - val_loss: 0.3992\n",
      "Epoch 9/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8114 - loss: 0.4106 - val_accuracy: 0.8100 - val_loss: 0.3971\n",
      "Epoch 10/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8162 - loss: 0.4006 - val_accuracy: 0.7940 - val_loss: 0.4097\n",
      "Epoch 11/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8378 - loss: 0.3665 - val_accuracy: 0.8260 - val_loss: 0.3800\n",
      "Epoch 12/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8403 - loss: 0.3505 - val_accuracy: 0.8205 - val_loss: 0.3841\n",
      "Epoch 13/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.8586 - loss: 0.3298 - val_accuracy: 0.8455 - val_loss: 0.3569\n",
      "Epoch 14/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8425 - loss: 0.3460 - val_accuracy: 0.8320 - val_loss: 0.3774\n",
      "Epoch 15/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8616 - loss: 0.3117 - val_accuracy: 0.8290 - val_loss: 0.3966\n",
      "Epoch 16/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8645 - loss: 0.3006 - val_accuracy: 0.8420 - val_loss: 0.3834\n",
      "Epoch 17/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8769 - loss: 0.2898 - val_accuracy: 0.8420 - val_loss: 0.3611\n",
      "Epoch 18/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8787 - loss: 0.2749 - val_accuracy: 0.8235 - val_loss: 0.4112\n",
      "Epoch 19/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.8870 - loss: 0.2682 - val_accuracy: 0.8375 - val_loss: 0.3680\n",
      "Epoch 20/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8940 - loss: 0.2518 - val_accuracy: 0.8395 - val_loss: 0.3869\n",
      "Epoch 21/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8899 - loss: 0.2507 - val_accuracy: 0.8345 - val_loss: 0.3889\n",
      "Epoch 22/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8989 - loss: 0.2337 - val_accuracy: 0.8490 - val_loss: 0.3972\n",
      "Epoch 23/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8988 - loss: 0.2351 - val_accuracy: 0.8500 - val_loss: 0.3726\n",
      "Epoch 24/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 37ms/step - accuracy: 0.9119 - loss: 0.2113 - val_accuracy: 0.8445 - val_loss: 0.4036\n",
      "Epoch 25/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.9137 - loss: 0.2126 - val_accuracy: 0.8365 - val_loss: 0.4059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e9d8171640>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(training_set, epochs=25, validation_data=test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
